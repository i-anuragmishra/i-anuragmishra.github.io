<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Anurag Mishra • AI Research Scientist</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,500;0,600;1,400&family=JetBrains+Mono:wght@400;500&family=Inter:wght@400;500;600&display=swap">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>

<body>
    <nav>
        <a href="index.html" class="active">~/home</a>
        <a href="publications.html">~/research</a>
        <a href="projects.html">~/projects</a>
        <a href="blogs.html">~/writing</a>
        <a href="Resume_Anurag_RIT.pdf">~/cv</a>
    </nav>

    <div class="profile">
        <h1>Hi, I'm Anurag Mishra</h1>
        <p>AI Research Scientist working at the intersection of explainable AI, ethics, and deep learning.</p>
        <p class="links">
            <a href="mailto:am2552@rit.edu">am2552@rit.edu</a> •
            <a href="https://github.com/i-anuragmishra">github</a> •
            <a href="https://www.linkedin.com/in/i-anuragmishra">linkedin</a> •
            <a href="https://scholar.google.com/citations?user=ZzAwD4EAAAAJ">scholar</a>
        </p>
    </div>

    <section>
        <p>
            I'm a graduate student and researcher at RIT, focusing on making AI systems more transparent and ethical.
            My work spans deepfake detection, bias mitigation in language models, and neural machine translation.
        </p>
        <p>
            Currently, I'm exploring the intersection of explainable AI and adversarial robustness,
            developing techniques to make AI systems both interpretable and resilient.
        </p>
    </section>

    <section>
        <h2 class="section-title">Research Focus</h2>

        <div class="research-area">
            <h3>→ Explainable AI (XAI) and Mechanistic Interpretability</h3>
            <p>Advancing research in explainability through mechanistic interpretability techniques, uncovering how
                models process and represent information. Developing frameworks to enhance the transparency and
                trustworthiness of AI systems across diverse applications.</p>
        </div>

        <div class="research-area">
            <h3>→ Detecting and Debiasing Large Language Models</h3>
            <p>Investigating biases in LLMs by analyzing their training data and architectures. Designing robust
                debiasing methodologies to ensure fair and ethical AI, with a focus on mitigating adverse impacts in
                real-world deployments.</p>
        </div>

        <div class="research-area">
            <h3>→ CUDA Kernel Development for High-Performance AI</h3>
            <p>Harnessing the power of CUDA for writing optimized kernels to accelerate computations in deep learning
                workflows. Exploring applications of CUDA in scaling and enhancing the efficiency of large language
                models for high-performance AI engineering.</p>
        </div>

        <div class="research-area">
            <h3>→ Deepfake Detection and Explainable AI</h3>
            <p>Building multi-modal deepfake detection systems using innovative approaches in neuro-symbolic AI and
                explainable AI. Enhancing detection accuracy and transparency with layer-wise relevance propagation to
                tackle emerging challenges in multimedia authenticity.</p>
        </div>
    </section>

    <section>
        <h2>Current Research Work</h2>

        <div class="item">
            <h3>DeFake Project</h3>
            <div class="item-meta">Machine Learning Research Assistant • Oct 2024 - Present</div>
            <p class="item-description">
                Leading research with Dr. Matthew Wright on explainable AI for deepfake detection.
                Developing novel approaches to make detection systems transparent and interpretable
                through multi-modal analysis and saliency mapping.
            </p>
        </div>

        <div class="item">
            <h3>LLM Security Research</h3>
            <div class="item-meta">Research Assistant @ RIT • Oct 2024 - Present</div>
            <p class="item-description">
                Collaborating with Dr. Bartosz Krawczyk to study adversarial vulnerabilities
                in large language models. Developing frameworks for bias detection and mitigation
                in AI-generated content.
            </p>
        </div>
    </section>

    <footer>
        <a href="mailto:am2552@rit.edu">email</a>
        <a href="https://github.com/i-anuragmishra">github</a>
        <a href="https://www.linkedin.com/in/i-anuragmishra">linkedin</a>
        <a href="https://scholar.google.com/citations?user=ZzAwD4EAAAAJ">scholar</a>
    </footer>
</body>

</html>